{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRCNN - Data augmentation\n",
    "This notebook implements the super-resolution convolutional neural network, SRCNN, as described by [Dong *et al.*](https://link.springer.com/chapter/10.1007/978-3-319-10593-2_13), but expands on the methodology by investigating further data augmentation including:\n",
    " * rotation\n",
    " * colour channel swapping\n",
    "\n",
    "This notebook only covers 2 scaling factors: $2\\times$ and $4\\times$. Results for each can be acquired by changing the scaling factor and re-running the notebook. See the **SRCNN - Baseline model** notebook for further information on the basic model and general methodology.\n",
    "\n",
    "## Summary\n",
    "Results from this notebook are summarized. Using $32 \\times 32$ training patches ($33 \\times 33$ for scaling factor 3), image patch stride of 14, training batch size of 64 and 500 epochs, training took roughly 5 minutes on a GTX 1080 Ti.\n",
    "\n",
    "| Scaling factor | Image Set | Bicubic - PSNR (mean) | SRCNN - PSNR (mean) | PSNR Mean Improvement | Bicubic - SSIM (mean) | SRCNN - SSIM (mean) | SSIM Mean Improvement |\n",
    "| :------------- | :-------: | :-------------------: | :-----------------: | :-------------------: | :-------------------: | :-----------------: | :-------------------: |\n",
    "| $2\\times$ | Set5 |$25.81$ dB | $26.84$ dB | $+1.02$ dB | $0.76$ | $0.79$ | $+0.03$ |\n",
    "| $2\\times$ | Set14 | $23.54$ dB | $24.33$ dB | $+0.79$ dB | $0.65$ | $0.69$ | $+0.04$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import site\n",
    "site.addsitedir('../')\n",
    "from srcnn_tf2.data.preprocessing import create_xy_patches, import_from_file, scale_batch, center_crop\n",
    "from srcnn_tf2.data.plotting import n_compare\n",
    "from srcnn_tf2.model.srcnn_model import SRCNN\n",
    "\n",
    "# Data locations.\n",
    "training_folder = '../../../sr_data/T91'\n",
    "set5_eval_folder = '../../../sr_data/Set5'\n",
    "set14_eval_folder = '../../../sr_data/Set14'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data augmentation: Rotation, channel swap, and alternative patching sizes\n",
    "### Model defintion\n",
    "Define the model first. Data must be imported on the fly as there are too many options to store the full training data set in memory at the same time (potentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, None, None, 128)   31232     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 64)    8256      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 64)    4160      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 3)     4803      \n",
      "=================================================================\n",
      "Total params: 48,451\n",
      "Trainable params: 48,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "srcnn_model = SRCNN(\n",
    "    num_channels=3,\n",
    "    f1=9,\n",
    "    f3=5,\n",
    "    n1=128,\n",
    "    n2=64,\n",
    "    nlin_layers=2,\n",
    "    activation='relu',\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['accuracy'],\n",
    "    padding='valid',\n",
    "    batch_norm=False\n",
    ")\n",
    "\n",
    "srcnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import, preprocess, and train\n",
    "High-resolution target images, $I_y$, are 32 x 32 pixel sub-images extracted from the original T91 image set. This is done by passing a 32 x 32 pixel window over the originals at a stride of 14 pixels. Target images are then given a Gaussian blur and downscaled by the scaling factor to produce the low-resolution input images, $I_x$. Pre-upscaling for the model is performed as part of the model class. \n",
    "\n",
    "#### Patching\n",
    "Three sizes of patches, all at a stride of 14 will be attemped: $32 \\times 32$, $48 \\times 48$, $64 \\times 64$\n",
    "\n",
    "#### Rotation\n",
    "\n",
    "#### Colour channel swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data options.\n",
    "# Note: 'scaling_factor' should evenly divide into 'y_image_size'.\n",
    "y_sizes = [(32, 32), (48, 48), (64, 64)]  # Target image size, patches extracted from T91 inputs.\n",
    "rotations = [0, 90, 180, 270]\n",
    "scaling_factor = 2\n",
    "patch_stride = 14\n",
    "blur_kernel = -1  # Negative applies blur before downscaling, positive applies blur after downscaling\n",
    "epochs_per_loop = 200\n",
    "batch_size = 32\n",
    "\n",
    "# Run training\n",
    "for y_image_size in y_sizes:\n",
    "    for rots in rotations:\n",
    "        # Data extraction\n",
    "        # ---------------\n",
    "        xdata, ydata = create_xy_patches(training_folder,\n",
    "                                         scaling_factor,\n",
    "                                         patch_size=y_image_size,\n",
    "                                         patch_stride=patch_stride,\n",
    "                                         blur_kernel=blur_kernel,\n",
    "                                         rotations=[0, 90, 180, 270], swap_channels=True)\n",
    "        print(f\"Rotation: {rots} | Image size: {y_image_size}\")\n",
    "        print(f\"Target data size (number of images x image shape x channels): {ydata.shape}\")\n",
    "        print(f\"Training data input size (number of images x image shape x channels): {xdata.shape}\")\n",
    "        \n",
    "        # Model training\n",
    "        # --------------\n",
    "        srcnn_model.fit(xdata=xdata, ydata=ydata, epochs=epochs_per_loop, batch_size=batch_size, validation_split=0.1, verbose=0)\n",
    "\n",
    "#srcnn_model.plot_training(figsize=(16, 8), plot_vars=['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "We evaluate the model using the images in Set5 and Set14. These are combined and run through our benchmarking method that measures both PSNR and SSIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_images_5 = import_from_file(set5_eval_folder)\n",
    "testing_images_14 = import_from_file(set14_eval_folder)\n",
    "\n",
    "psnr_5, psnr_bicubic_5 = srcnn_model.benchmark(testing_images_5, metric='psnr', return_metrics=True)\n",
    "psnr_14, psnr_bicubic_14 = srcnn_model.benchmark(testing_images_14, metric='psnr', return_metrics=True)\n",
    "ssim_5, ssim_bicubic_5 = srcnn_model.benchmark(testing_images_5, metric='ssim', return_metrics=True)\n",
    "ssim_14, ssim_bicubic_14 = srcnn_model.benchmark(testing_images_14, metric='ssim', return_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#srcnn_model.benchmark(testing_images_5, metric='psnr', return_metrics=False)\n",
    "#srcnn_model.benchmark(testing_images_14, metric='psnr', return_metrics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#srcnn_model.benchmark(testing_images_5, metric='ssim', return_metrics=False)\n",
    "#srcnn_model.benchmark(testing_images_14, metric='ssim', return_metrics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"| Scaling factor | Image Set | Bicubic - PSNR (mean) | SRCNN - PSNR (mean) | PSNR Mean Improvement | Bicubic - SSIM (mean) | SRCNN - SSIM (mean) | SSIM Mean Improvement |\")\n",
    "print(f\"| :------------- | :-------: | :-------------------: | :-----------------: | :-------------------: | :-------------------: | :-----------------: | :-------------------: |\")\n",
    "print(f\"| ${scaling_factor}x$ | Set5 |${np.mean(psnr_bicubic_5):.2f}$ dB | ${np.mean(psnr_5):.2f}$ dB | $+{np.mean(psnr_5) - np.mean(psnr_bicubic_5):.2f}$ dB |\", end='')\n",
    "print(f\" ${np.mean(ssim_bicubic_5):.2f}$ | ${np.mean(ssim_5):.2f}$ | $+{np.mean(ssim_5) - np.mean(ssim_bicubic_5):.2f}$ |\")\n",
    "print(f\"| ${scaling_factor}x$ | Set14 | ${np.mean(psnr_bicubic_14):.2f}$ dB | ${np.mean(psnr_14):.2f}$ dB | $+{np.mean(psnr_14) - np.mean(psnr_bicubic_14):.2f}$ dB |\", end='')\n",
    "print(f\" ${np.mean(ssim_bicubic_14):.2f}$ | ${np.mean(ssim_14):.2f}$ | $+{np.mean(ssim_14) - np.mean(ssim_bicubic_14):.2f}$ |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### CIFAR-10 Examples\n",
    "We use the CIFAR-10 dataset as a \"real world\" application where there is no target with which to compare. We upscale using both the trained SRCNN and bicubic interpolation, and compare visually (there is no metric in this case).\n",
    "\n",
    "Images are saved with tags so they can be included in the summary and compare across the three scaling factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "\n",
    "# Data import and definition.\n",
    "d_example_index = {'airplane': 30,\n",
    "                   'automobile': 32,\n",
    "                   'bird': 90,\n",
    "                   'cat': 91,\n",
    "                   'deer': 130,\n",
    "                   'dog': 156,\n",
    "                   'frog': 72,\n",
    "                   'horse': 152,\n",
    "                   'ship': 62,\n",
    "                   'truck': 122}\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "del x_test\n",
    "del y_test\n",
    "del y_train\n",
    "\n",
    "label_list, im_list = [], []\n",
    "for di in d_example_index.keys():\n",
    "    label_list.append(di)\n",
    "    im_list.append(x_train[d_example_index[di]])\n",
    "\n",
    "im_list = np.array(im_list) / 255.0\n",
    "\n",
    "del x_train\n",
    "# Save image string.\n",
    "im_prefix = f'srcnn_rotation_{scaling_factor}x_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pred = srcnn_model.predict(im_list)\n",
    "im_scale = center_crop(\n",
    "    images=scale_batch(im_list, (im_list.shape[2]*scaling_factor, im_list.shape[1]*scaling_factor)),\n",
    "    remove_edge=(im_list.shape[1]*scaling_factor - im_pred.shape[1])//2)\n",
    "\n",
    "print((im_list.shape[1]*scaling_factor - im_pred.shape[1])//2, im_list.shape[1]*scaling_factor, im_pred.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, image_raw, image_pred, image_scale in zip(label_list, list(im_list), list(im_pred), list(im_scale)):\n",
    "    n_compare(\n",
    "        im_list=[image_raw, image_scale, image_pred],\n",
    "        label_list=[f'Original: {label.upper()} - [{image_raw.shape[1]} x {image_raw.shape[0]}]',\n",
    "                    f'Bicubic Interpolation x{scaling_factor} - [{image_scale.shape[1]} x {image_scale.shape[0]}]',\n",
    "                    f'SRCNN x{scaling_factor} - [{image_pred.shape[1]} x {image_pred.shape[0]}]'],\n",
    "        figsize=(12,5))\n",
    "    \n",
    "    #im = Image.fromarray(np.uint8(image_pred*255))\n",
    "    #im.save(f\"results/{im_prefix}{label}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
