{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRCNN - Data augmentation\n",
    "This notebook implements the super-resolution convolutional neural network, SRCNN, as described by [Dong *et al.*](https://link.springer.com/chapter/10.1007/978-3-319-10593-2_13), but expands on the methodology by investigating further data augmentation including:\n",
    " * rotation\n",
    " * colour channel swapping\n",
    "\n",
    "This notebook only covers 2 scaling factors: $2\\times$ and $4\\times$. Results for each can be acquired by changing the scaling factor and re-running the notebook. See the **SRCNN - Baseline model** notebook for further information on the basic model and general methodology.\n",
    "\n",
    "## Summary\n",
    "Results from this notebook are summarized. Using $32 \\times 32$ training patches ($33 \\times 33$ for scaling factor 3), image patch stride of 14, training batch size of 64 and 500 epochs, training took roughly 5 minutes on a GTX 1080 Ti.\n",
    "\n",
    "| Scaling factor | Image Set | Rotations | Channel Swap | Multi-size |  Bicubic - PSNR (mean) | SRCNN - PSNR (mean) | PSNR Mean Improvement | Bicubic - SSIM (mean) | SRCNN - SSIM (mean) | SSIM Mean Improvement |\n",
    "| :------------- | :-------: | :-------: | :----------: | :--------: | :-------------------: | :-----------------: | :-------------------: | :-------------------: | :-----------------: | :-------------------: |\n",
    "| $2\\times$ | Set14 | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | $23.54$ dB | $24.36$ dB | $+0.82$ dB | $0.65$ | $0.69$ | $+0.04$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import os\n",
    "import shutil\n",
    "from random import shuffle\n",
    "\n",
    "import site\n",
    "site.addsitedir('../')\n",
    "from srcnn_tf2.data.preprocessing import create_xy_patches, import_from_file, scale_batch, center_crop\n",
    "from srcnn_tf2.data.plotting import n_compare\n",
    "from srcnn_tf2.data.oom import SRCNNTrainingGenerator\n",
    "from srcnn_tf2.model.srcnn_model import SRCNN\n",
    "\n",
    "# Data locations.\n",
    "training_folder = '../../../sr_data/T91'\n",
    "set5_eval_folder = '../../../sr_data/Set5'\n",
    "set14_eval_folder = '../../../sr_data/Set14'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data augmentation: Rotation, channel swap, and alternative patching sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data options.\n",
    "# Note: 'scaling_factor' should evenly divide into 'y_image_size'.\n",
    "y_sizes = [(32, 32), (48, 48), (64, 64)]  # Target image size, patches extracted from T91 inputs.\n",
    "rotations = [0, 90, 180, 270]\n",
    "channel_combos = [(0,1,2), (1,2,0), (2,0,1)]\n",
    "scaling_factor = 2\n",
    "patch_stride = 14\n",
    "blur_kernel = -1  # Negative applies blur before downscaling, positive applies blur after downscaling\n",
    "epochs_per_loop = 100\n",
    "batch_size = 32\n",
    "\n",
    "y_folder = '../../../sr_data/srcnn_training_temp/ydata'\n",
    "x_folder = '../../../sr_data/srcnn_training_temp/xdata'\n",
    "\n",
    "shutil.rmtree(y_folder)\n",
    "os.makedirs(y_folder)\n",
    "shutil.rmtree(x_folder)\n",
    "os.makedirs(x_folder)\n",
    "\n",
    "# Build data on disk.\n",
    "counter = 0\n",
    "filenames = []\n",
    "for y_image_size in y_sizes:\n",
    "    for rots in rotations:\n",
    "        for channels in channel_combos:\n",
    "            # Data extraction\n",
    "            # ---------------\n",
    "            xdata, ydata = create_xy_patches(training_folder,\n",
    "                                             scaling_factor,\n",
    "                                             patch_size=y_image_size,\n",
    "                                             patch_stride=patch_stride,\n",
    "                                             blur_kernel=blur_kernel,\n",
    "                                             rotations=[rots], swap_channels=channels)\n",
    "            xdata = scale_batch(xdata, ydata.shape[1:3])\n",
    "            ydata = ydata if srcnn_model.padding=='same' else center_crop(ydata, srcnn_model.get_crop_size())\n",
    "            xdata, ydata = xdata[:10], ydata[:10]\n",
    "            for xd, yd in zip(xdata, ydata):\n",
    "                im_size_str = f\"{y_image_size.shape[0]}x{y_image_size.shape[1]}\"\n",
    "                rot_str = f\"{rots}deg\"\n",
    "                chan_str = f\"c{str(channels[0])+str(channels[1])+str(channels[2])}\"\n",
    "                filename_x = f\"{x_folder}/{counter}_{im_size_str}_{rot_str}_{chan_str}_x.npy\"\n",
    "                filename_y = f\"{y_folder}/{counter}_{im_size_str}_{rot_str}_{chan_str}_y.npy\"\n",
    "                filenames.append(filename_x, filename_y)\n",
    "                np.save(filename_x, xd)\n",
    "                np.save(filename_y, yd)\n",
    "                counter += 1\n",
    "\n",
    "print(f\"Training data has been augmented and saved to disk. {len(filenames)} images have been saved.\")\n",
    "\n",
    "shuffle(filenames) # Shuffle the file names\n",
    "\n",
    "filenames_rotation_only = [f for f in filenames if ('_c012' in f[0]) & ('_32x32' in f[0])]\n",
    "filenames_chanswap_only = [f for f in filenames if ('_90deg' in f[0]) & ('_32x32' in f[0])]\n",
    "filenames_imsize_only = [f for f in filenames if ('_c012' in f[0]) & ('_90deg' in f[0])]\n",
    "\n",
    "# Test list contains tuples of the form (rotations, channel swap, image size, filenames)\n",
    "test_list = [\n",
    "    (True, False, False, filenames_rotation_only),\n",
    "    (False, True, False, filenames_chanswap_only),\n",
    "    (False, False, True, filenames_imsize_only),\n",
    "    (True, True, True, filenames)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_strings = []\n",
    "testing_images_14 = import_from_file(set14_eval_folder)\n",
    "\n",
    "for rot, chan, im, files in test_list:\n",
    "    # Build new model.\n",
    "    srcnn_m = SRCNN(num_channels=3, f1=9, f3=5, n1=64, n2=32, nlin_layers=1,\n",
    "                    activation='relu', optimizer='adam', loss='mse', metrics=['accuracy'], padding='valid', batch_norm=False)\n",
    "    \n",
    "    # Instantiate generator and train.\n",
    "    train_gen = SRCNNTrainingGenerator(files, 64)\n",
    "    srcnn_m.model.fit_generator(train_gen)\n",
    "    \n",
    "    # Evaluate performance.\n",
    "    psnr_14, psnr_bicubic_14 = srcnn_m.benchmark(testing_images_14, metric='psnr', return_metrics=True)\n",
    "    ssim_14, ssim_bicubic_14 = srcnn_m.benchmark(testing_images_14, metric='ssim', return_metrics=True)\n",
    "    \n",
    "    # Record performance.\n",
    "    rot_check = '$\\\\checkmark$' if rot > 1 else ''\n",
    "    chan_check = '$\\\\checkmark$' if chan > 1 else ''\n",
    "    mult_check = '$\\\\checkmark$' if im > 1 else ''\n",
    "    result_strings.append(\n",
    "        f\"| ${scaling_factor}\\\\times$ | Set14 | {rot_check} | {chan_check} | {mult_check} |\"+\n",
    "        f\" ${np.mean(psnr_bicubic_14):.2f}$ dB | ${np.mean(psnr_14):.2f}$ dB |\"+\n",
    "        f\" $+{np.mean(psnr_14) - np.mean(psnr_bicubic_14):.2f}$ dB |\"+\n",
    "        f\" ${np.mean(ssim_bicubic_14):.2f}$ | ${np.mean(ssim_14):.2f}$ | $+{np.mean(ssim_14) - np.mean(ssim_bicubic_14):.2f}$ |\"\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"| Scaling factor | Image Set | Rotations | Channel Swap | Multi-size |  Bicubic - PSNR (mean) | SRCNN - PSNR (mean) | PSNR Mean Improvement | Bicubic - SSIM (mean) | SRCNN - SSIM (mean) | SSIM Mean Improvement |\")\n",
    "print(f\"| :------------- | :-------: | :-------: | :----------: | :--------: | :-------------------: | :-----------------: | :-------------------: | :-------------------: | :-----------------: | :-------------------: |\")\n",
    "for p in result_strings:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model defintion\n",
    "Define the model first. Data must be imported on the fly as there are too many options to store the full training data set in memory at the same time (potentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, None, None, 256)   93184     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 128)   32896     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 128)   16512     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 128)   16512     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, None, None, 128)   16512     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, None, None, 3)     18819     \n",
      "=================================================================\n",
      "Total params: 194,435\n",
      "Trainable params: 194,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "srcnn_model = SRCNN(\n",
    "    num_channels=3,\n",
    "    f1=11,\n",
    "    f3=7,\n",
    "    n1=256,\n",
    "    n2=128,\n",
    "    nlin_layers=4,\n",
    "    activation='relu',\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['accuracy'],\n",
    "    padding='valid',\n",
    "    batch_norm=False\n",
    ")\n",
    "\n",
    "srcnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and preprocess\n",
    "High-resolution target images, $I_y$, are 32 x 32 pixel sub-images extracted from the original T91 image set. This is done by passing a 32 x 32 pixel window over the originals at a stride of 14 pixels. Target images are then given a Gaussian blur and downscaled by the scaling factor to produce the low-resolution input images, $I_x$. Pre-upscaling for the model is performed as part of the model class.\n",
    "\n",
    "Training data is saved to disk temporarily as it is too large to hold the complete set in memory. A custom generator is built to pull data from disk and the `.fit_generator()` method will have to be called directly on the model. Pre-upscaling will be done *outside* the SRCNN class (unlike with in-memory data).\n",
    "\n",
    "#### Multi-size patching\n",
    "Three sizes of patches, all at a stride of 14 will be attemped: $32 \\times 32$, $48 \\times 48$, $64 \\times 64$\n",
    "\n",
    "#### Rotation\n",
    "\n",
    "#### Colour channel swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rotation: 0 | Image size: (32, 32) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 22.86 seconds, approx. 3.23 seconds per epoch.\n",
      "\n",
      "Rotation: 0 | Image size: (32, 32) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 21.87 seconds, approx. 3.22 seconds per epoch.\n",
      "\n",
      "Rotation: 0 | Image size: (32, 32) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 24.43 seconds, approx. 3.24 seconds per epoch.\n",
      "\n",
      "Rotation: 90 | Image size: (32, 32) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 20.76 seconds, approx. 3.21 seconds per epoch.\n",
      "\n",
      "Rotation: 90 | Image size: (32, 32) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 20.50 seconds, approx. 3.20 seconds per epoch.\n",
      "\n",
      "Rotation: 90 | Image size: (32, 32) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 24.05 seconds, approx. 3.24 seconds per epoch.\n",
      "\n",
      "Rotation: 180 | Image size: (32, 32) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 21.56 seconds, approx. 3.22 seconds per epoch.\n",
      "\n",
      "Rotation: 180 | Image size: (32, 32) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 22.08 seconds, approx. 3.22 seconds per epoch.\n",
      "\n",
      "Rotation: 180 | Image size: (32, 32) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 24.16 seconds, approx. 3.24 seconds per epoch.\n",
      "\n",
      "Rotation: 270 | Image size: (32, 32) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 21.40 seconds, approx. 3.21 seconds per epoch.\n",
      "\n",
      "Rotation: 270 | Image size: (32, 32) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 23.40 seconds, approx. 3.23 seconds per epoch.\n",
      "\n",
      "Rotation: 270 | Image size: (32, 32) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (22623, 32, 32, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (22623, 8, 8, 3)\n",
      "100 epochs completed in 5.0 minutes 19.03 seconds, approx. 3.19 seconds per epoch.\n",
      "\n",
      "Rotation: 0 | Image size: (48, 48) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 16.71 seconds, approx. 7.37 seconds per epoch.\n",
      "\n",
      "Rotation: 0 | Image size: (48, 48) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 16.14 seconds, approx. 7.36 seconds per epoch.\n",
      "\n",
      "Rotation: 0 | Image size: (48, 48) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 16.19 seconds, approx. 7.36 seconds per epoch.\n",
      "\n",
      "Rotation: 90 | Image size: (48, 48) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 16.57 seconds, approx. 7.37 seconds per epoch.\n",
      "\n",
      "Rotation: 90 | Image size: (48, 48) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 17.14 seconds, approx. 7.37 seconds per epoch.\n",
      "\n",
      "Rotation: 90 | Image size: (48, 48) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 20.12 seconds, approx. 7.40 seconds per epoch.\n",
      "\n",
      "Rotation: 180 | Image size: (48, 48) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 16.03 seconds, approx. 7.36 seconds per epoch.\n",
      "\n",
      "Rotation: 180 | Image size: (48, 48) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 17.21 seconds, approx. 7.37 seconds per epoch.\n",
      "\n",
      "Rotation: 180 | Image size: (48, 48) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 17.33 seconds, approx. 7.37 seconds per epoch.\n",
      "\n",
      "Rotation: 270 | Image size: (48, 48) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 16.94 seconds, approx. 7.37 seconds per epoch.\n",
      "\n",
      "Rotation: 270 | Image size: (48, 48) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 20.69 seconds, approx. 7.41 seconds per epoch.\n",
      "\n",
      "Rotation: 270 | Image size: (48, 48) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (19598, 48, 48, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (19598, 12, 12, 3)\n",
      "100 epochs completed in 12.0 minutes 15.80 seconds, approx. 7.36 seconds per epoch.\n",
      "\n",
      "Rotation: 0 | Image size: (64, 64) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (16874, 64, 64, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (16874, 16, 16, 3)\n",
      "100 epochs completed in 17.0 minutes 32.31 seconds, approx. 10.52 seconds per epoch.\n",
      "\n",
      "Rotation: 0 | Image size: (64, 64) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (16874, 64, 64, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (16874, 16, 16, 3)\n",
      "100 epochs completed in 17.0 minutes 37.42 seconds, approx. 10.57 seconds per epoch.\n",
      "\n",
      "Rotation: 0 | Image size: (64, 64) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (16874, 64, 64, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (16874, 16, 16, 3)\n",
      "100 epochs completed in 17.0 minutes 30.22 seconds, approx. 10.50 seconds per epoch.\n",
      "\n",
      "Rotation: 90 | Image size: (64, 64) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (16874, 64, 64, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (16874, 16, 16, 3)\n",
      "100 epochs completed in 17.0 minutes 34.47 seconds, approx. 10.54 seconds per epoch.\n",
      "\n",
      "Rotation: 90 | Image size: (64, 64) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (16874, 64, 64, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (16874, 16, 16, 3)\n",
      "100 epochs completed in 17.0 minutes 23.80 seconds, approx. 10.44 seconds per epoch.\n",
      "\n",
      "Rotation: 90 | Image size: (64, 64) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (16874, 64, 64, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (16874, 16, 16, 3)\n",
      "100 epochs completed in 17.0 minutes 18.38 seconds, approx. 10.38 seconds per epoch.\n",
      "\n",
      "Rotation: 180 | Image size: (64, 64) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (16874, 64, 64, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (16874, 16, 16, 3)\n",
      "100 epochs completed in 17.0 minutes 23.18 seconds, approx. 10.43 seconds per epoch.\n",
      "\n",
      "Rotation: 180 | Image size: (64, 64) | Channel order: (1, 2, 0)\n",
      "\tTarget data size (number of images x image shape x channels): (16874, 64, 64, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (16874, 16, 16, 3)\n",
      "100 epochs completed in 17.0 minutes 17.22 seconds, approx. 10.37 seconds per epoch.\n",
      "\n",
      "Rotation: 180 | Image size: (64, 64) | Channel order: (2, 0, 1)\n",
      "\tTarget data size (number of images x image shape x channels): (16874, 64, 64, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (16874, 16, 16, 3)\n",
      "100 epochs completed in 17.0 minutes 9.44 seconds, approx. 10.29 seconds per epoch.\n",
      "\n",
      "Rotation: 270 | Image size: (64, 64) | Channel order: (0, 1, 2)\n",
      "\tTarget data size (number of images x image shape x channels): (16874, 64, 64, 3)\n",
      "\tTraining data input size (number of images x image shape x channels): (16874, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "# Data options.\n",
    "# Note: 'scaling_factor' should evenly divide into 'y_image_size'.\n",
    "y_sizes = [(32, 32), (48, 48), (64, 64)]  # Target image size, patches extracted from T91 inputs.\n",
    "rotations = [0, 90, 180, 270]\n",
    "channel_combos = [(0,1,2), (1,2,0), (2,0,1)]\n",
    "scaling_factor = 4\n",
    "patch_stride = 14\n",
    "blur_kernel = -1  # Negative applies blur before downscaling, positive applies blur after downscaling\n",
    "epochs_per_loop = 100\n",
    "batch_size = 32\n",
    "\n",
    "y_folder = '../../../sr_data/srcnn_training_temp/ydata'\n",
    "x_folder = '../../../sr_data/srcnn_training_temp/xdata'\n",
    "\n",
    "# Run training\n",
    "for y_image_size in y_sizes:\n",
    "    for rots in rotations:\n",
    "        for channels in channel_combos:\n",
    "            # Data extraction\n",
    "            # ---------------\n",
    "            xdata, ydata = create_xy_patches(training_folder,\n",
    "                                             scaling_factor,\n",
    "                                             patch_size=y_image_size,\n",
    "                                             patch_stride=patch_stride,\n",
    "                                             blur_kernel=blur_kernel,\n",
    "                                             rotations=[rots], swap_channels=channels)\n",
    "            print(f\"\\nRotation: {rots} | Image size: {y_image_size} | Channel order: {channels}\")\n",
    "            print(f\"\\tTarget data size (number of images x image shape x channels): {ydata.shape}\")\n",
    "            print(f\"\\tTraining data input size (number of images x image shape x channels): {xdata.shape}\")\n",
    "            \n",
    "            # Model training\n",
    "            # --------------\n",
    "            srcnn_model.fit(xdata=xdata, ydata=ydata, epochs=epochs_per_loop, batch_size=batch_size, validation_split=0.1, verbose=0)\n",
    "            \n",
    "\n",
    "#srcnn_model.plot_training(figsize=(16, 8), plot_vars=['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "We evaluate the model using the images on Set14 alone. These run through our benchmarking method that measures both PSNR and SSIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_images_5 = import_from_file(set5_eval_folder)\n",
    "testing_images_14 = import_from_file(set14_eval_folder)\n",
    "\n",
    "#psnr_5, psnr_bicubic_5 = srcnn_model.benchmark(testing_images_5, metric='psnr', return_metrics=True)\n",
    "psnr_14, psnr_bicubic_14 = srcnn_model.benchmark(testing_images_14, metric='psnr', return_metrics=True)\n",
    "#ssim_5, ssim_bicubic_5 = srcnn_model.benchmark(testing_images_5, metric='ssim', return_metrics=True)\n",
    "ssim_14, ssim_bicubic_14 = srcnn_model.benchmark(testing_images_14, metric='ssim', return_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#srcnn_model.benchmark(testing_images_5, metric='psnr', return_metrics=False)\n",
    "#srcnn_model.benchmark(testing_images_14, metric='psnr', return_metrics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#srcnn_model.benchmark(testing_images_5, metric='ssim', return_metrics=False)\n",
    "#srcnn_model.benchmark(testing_images_14, metric='ssim', return_metrics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_check = '$\\\\checkmark$' if len(rotations) > 1 else ''\n",
    "chan_check = '$\\\\checkmark$' if len(channel_combos) > 1 else ''\n",
    "mult_check = '$\\\\checkmark$' if len(y_sizes) > 1 else ''\n",
    "\n",
    "print(f\"| Scaling factor | Image Set | Rotations | Channel Swap | Multi-size |  Bicubic - PSNR (mean) | SRCNN - PSNR (mean) | PSNR Mean Improvement | Bicubic - SSIM (mean) | SRCNN - SSIM (mean) | SSIM Mean Improvement |\")\n",
    "print(f\"| :------------- | :-------: | :-------: | :----------: | :--------: | :-------------------: | :-----------------: | :-------------------: | :-------------------: | :-----------------: | :-------------------: |\")\n",
    "print(f\"| ${scaling_factor}\\\\times$ | Set14 | {rot_check} | {chan_check} | {mult_check} | ${np.mean(psnr_bicubic_14):.2f}$ dB | ${np.mean(psnr_14):.2f}$ dB | $+{np.mean(psnr_14) - np.mean(psnr_bicubic_14):.2f}$ dB |\", end='')\n",
    "print(f\" ${np.mean(ssim_bicubic_14):.2f}$ | ${np.mean(ssim_14):.2f}$ | $+{np.mean(ssim_14) - np.mean(ssim_bicubic_14):.2f}$ |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### CIFAR-10 Examples\n",
    "We use the CIFAR-10 dataset as a \"real world\" application where there is no target with which to compare. We upscale using both the trained SRCNN and bicubic interpolation, and compare visually (there is no metric in this case).\n",
    "\n",
    "Images are saved with tags so they can be included in the summary and compare across the three scaling factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "\n",
    "# Data import and definition.\n",
    "d_example_index = {'airplane': 30,\n",
    "                   'automobile': 32,\n",
    "                   'bird': 90,\n",
    "                   'cat': 91,\n",
    "                   'deer': 130,\n",
    "                   'dog': 156,\n",
    "                   'frog': 72,\n",
    "                   'horse': 152,\n",
    "                   'ship': 62,\n",
    "                   'truck': 122}\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "del x_test\n",
    "del y_test\n",
    "del y_train\n",
    "\n",
    "label_list, im_list = [], []\n",
    "for di in d_example_index.keys():\n",
    "    label_list.append(di)\n",
    "    im_list.append(x_train[d_example_index[di]])\n",
    "\n",
    "im_list = np.array(im_list) / 255.0\n",
    "\n",
    "del x_train\n",
    "# Save image string.\n",
    "im_prefix = f'srcnn_rotation_channelSwap_multisizeImage_{scaling_factor}x_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pred = srcnn_model.predict(im_list)\n",
    "im_scale = center_crop(\n",
    "    images=scale_batch(im_list, (im_list.shape[2]*scaling_factor, im_list.shape[1]*scaling_factor)),\n",
    "    remove_edge=(im_list.shape[1]*scaling_factor - im_pred.shape[1])//2)\n",
    "\n",
    "print((im_list.shape[1]*scaling_factor - im_pred.shape[1])//2, im_list.shape[1]*scaling_factor, im_pred.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, image_raw, image_pred, image_scale in zip(label_list, list(im_list), list(im_pred), list(im_scale)):\n",
    "    n_compare(\n",
    "        im_list=[image_raw, image_scale, image_pred],\n",
    "        label_list=[f'Original: {label.upper()} - [{image_raw.shape[1]} x {image_raw.shape[0]}]',\n",
    "                    f'Bicubic Interpolation x{scaling_factor} - [{image_scale.shape[1]} x {image_scale.shape[0]}]',\n",
    "                    f'SRCNN x{scaling_factor} - [{image_pred.shape[1]} x {image_pred.shape[0]}]'],\n",
    "        figsize=(12,5))\n",
    "    \n",
    "    #im = Image.fromarray(np.uint8(image_pred*255))\n",
    "    #im.save(f\"results/{im_prefix}{label}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
